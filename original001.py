# -*- coding: utf-8 -*-
"""original001.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vf4_tpmsFb5Pl_VMKv13Rf6cf6S67ryW
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# 
# #installation libraries
# !pip install pafy youtube-dl moviepy

from google.colab import drive
drive.mount('/content/drive')

from google.colab import files
import os

# Upload a video file
uploaded = files.upload()

# Get the filename of the uploaded file
video_filename = list(uploaded.keys())[0]
video_path = os.path.join("/content", video_filename)  # Save path

print(f"‚úÖ Video uploaded successfully: {video_filename}")

import os
import cv2
import random
import numpy as np

# Constants
SEQUENCE_LENGTH = 20  # Number of frames per video sequence
IMAGE_HEIGHT, IMAGE_WIDTH = 128, 128  # Frame dimensions
DATASET_DIR = "/content/drive/MyDrive/UCF4"  # Correct dataset path

# Dynamically generate class list from dataset directory
CLASSES_LIST = [folder for folder in os.listdir(DATASET_DIR) if os.path.isdir(os.path.join(DATASET_DIR, folder))]

print("Classes found in the dataset:")
for class_name in CLASSES_LIST:
    print(class_name)

def frames_extraction(video_path):
    """
    Extract a fixed number of frames (SEQUENCE_LENGTH) from a video.
    """
    frames_list = []
    video_reader = cv2.VideoCapture(video_path)
    video_frames_count = 0

    # Count frames manually if CAP_PROP_FRAMES_COUNT isn't available
    while True:
        success, _ = video_reader.read()
        if not success:
            break
        video_frames_count += 1

    video_reader.release()

    # Reopen video for frame extraction
    video_reader = cv2.VideoCapture(video_path)
    skip_frames_window = max(int(video_frames_count / SEQUENCE_LENGTH), 1)

    for frame_counter in range(SEQUENCE_LENGTH):
        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)
        success, frame = video_reader.read()
        if not success:
            break

        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        normalized_frame = resized_frame / 255.0
        frames_list.append(normalized_frame)

    video_reader.release()
    return frames_list

def create_dataset():
    """
    Create the dataset by extracting frames from all video files in the dataset directory.
    """
    features = []
    labels = []
    video_file_paths = []

    for class_index, class_name in enumerate(CLASSES_LIST):
        print(f"Extracting data for class: {class_name}")
        class_dir = os.path.join(DATASET_DIR, class_name)

        # Traverse all subdirectories and collect video file paths
        for root, _, files in os.walk(class_dir):
            for file_name in files:
                if file_name.endswith(('.avi', '.mp4', '.mkv')):  # Adjust extensions as needed
                    video_file_path = os.path.join(root, file_name)
                    frames = frames_extraction(video_file_path)
                    if len(frames) == SEQUENCE_LENGTH:
                        features.append(frames)
                        labels.append(class_index)
                        video_file_paths.append(video_file_path)

    features = np.asarray(features)
    labels = np.array(labels)
    return features, labels, video_file_paths

# Generate the dataset
features, labels, video_file_paths = create_dataset()

print(f"Dataset created with {len(features)} samples.")

# Import required libraries
import os
import cv2
import random
import numpy as np
import datetime as dt
from sklearn.model_selection import train_test_split
from tensorflow.keras.layers import *
from tensorflow.keras.models import Sequential
from tensorflow.keras.utils import to_categorical, plot_model
from tensorflow.keras.callbacks import EarlyStopping

# Fix random seeds for reproducibility
seed_constant = 27
np.random.seed(seed_constant)
random.seed(seed_constant)
import tensorflow as tf
tf.random.set_seed(seed_constant)

# Constants
SEQUENCE_LENGTH = 20  # Number of frames per video sequence
IMAGE_HEIGHT, IMAGE_WIDTH = 128, 128  # Frame dimensions
DATASET_DIR = "/content/drive/MyDrive/UCF4"  # Dataset path

# Dynamically generate class list
CLASSES_LIST = [folder for folder in os.listdir(DATASET_DIR) if os.path.isdir(os.path.join(DATASET_DIR, folder))]

# Function to extract frames from a video
def frames_extraction(video_path):
    frames_list = []
    video_reader = cv2.VideoCapture(video_path)
    video_frames_count = 0

    # Count frames manually
    while True:
        success, _ = video_reader.read()
        if not success:
            break
        video_frames_count += 1

    video_reader.release()

    # Reopen video for extraction
    video_reader = cv2.VideoCapture(video_path)
    skip_frames_window = max(int(video_frames_count / SEQUENCE_LENGTH), 1)

    for frame_counter in range(SEQUENCE_LENGTH):
        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)
        success, frame = video_reader.read()
        if not success:
            break

        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        normalized_frame = resized_frame / 255.0
        frames_list.append(normalized_frame)

    video_reader.release()
    return frames_list

# Function to create the dataset
def create_dataset():
    features = []
    labels = []
    video_file_paths = []

    for class_index, class_name in enumerate(CLASSES_LIST):
        print(f"Extracting data for class: {class_name}")
        class_dir = os.path.join(DATASET_DIR, class_name)

        for root, _, files in os.walk(class_dir):
            for file_name in files:
                if file_name.endswith(('.avi', '.mp4', '.mkv')):
                    video_file_path = os.path.join(root, file_name)
                    frames = frames_extraction(video_file_path)
                    if len(frames) == SEQUENCE_LENGTH:
                        features.append(frames)
                        labels.append(class_index)
                        video_file_paths.append(video_file_path)

    features = np.asarray(features)
    labels = np.array(labels)
    return features, labels, video_file_paths

# Generate the dataset
features, labels, video_file_paths = create_dataset()

# One-hot encode the labels
one_hot_encoded_labels = to_categorical(labels)

# Split dataset into training and testing
features_train, features_test, labels_train, labels_test = train_test_split(
    features, one_hot_encoded_labels, test_size=0.25, shuffle=True, random_state=seed_constant
)

# Define the ConvLSTM model
# Define the ConvLSTM model
def create_convlstm_model():
    model = Sequential()

    # ConvLSTM layer 1
    model.add(ConvLSTM2D(filters=4, kernel_size=(3, 3), activation='tanh',
                         data_format="channels_last", recurrent_dropout=0.2,
                         return_sequences=True, input_shape=(SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 3)))
    model.add(MaxPooling3D(pool_size=(1, 2, 2), padding='same', data_format='channels_last'))
    model.add(TimeDistributed(Dropout(0.2)))

    # ConvLSTM layer 2
    model.add(ConvLSTM2D(filters=8, kernel_size=(3, 3), activation='tanh',
                         data_format="channels_last", recurrent_dropout=0.2,
                         return_sequences=True))
    model.add(MaxPooling3D(pool_size=(1, 2, 2), padding='same', data_format='channels_last'))
    model.add(TimeDistributed(Dropout(0.2)))

    # ConvLSTM layer 3
    model.add(ConvLSTM2D(filters=14, kernel_size=(3, 3), activation='tanh',
                         data_format="channels_last", recurrent_dropout=0.2,
                         return_sequences=True))
    model.add(MaxPooling3D(pool_size=(1, 2, 2), padding='same', data_format='channels_last'))
    model.add(TimeDistributed(Dropout(0.2)))

    # ConvLSTM layer 4
    model.add(ConvLSTM2D(filters=16, kernel_size=(3, 3), activation='tanh',
                         data_format="channels_last", recurrent_dropout=0.2,
                         return_sequences=False))

    # Remove MaxPooling3D, replace with Flatten
    model.add(Flatten())

    # Fully connected layer
    model.add(Dense(len(CLASSES_LIST), activation="softmax"))

    model.summary()
    return model

# Create the model
convlstm_model = create_convlstm_model()
print("Model created successfully")

# Plot the model architecture
plot_model(convlstm_model, to_file='convlstm_model_structure_plot.png', show_shapes=True, show_layer_names=True)

# Compile the model
early_stopping_callback = EarlyStopping(
    monitor='val_loss', patience=10, mode='min', restore_best_weights=True
)
convlstm_model.compile(
    loss='categorical_crossentropy', optimizer='Adam', metrics=["accuracy"]
)

# Train the model
convlstm_model_training_history = convlstm_model.fit(
    x=features_train, y=labels_train, epochs=50, batch_size=4,
    shuffle=True, validation_split=0.2, callbacks=[early_stopping_callback]
)

# Evaluate the model
model_evaluation_loss, model_evaluation_accuracy = convlstm_model.evaluate(features_test, labels_test)

# Save the model
date_time_format = '%Y_%m_%d'
current_date_time_dt = dt.datetime.now()
current_date_time_string = dt.datetime.strftime(current_date_time_dt, date_time_format)

model_file_name = f'convlstm_model__Date_Time_{current_date_time_string}__Loss_{model_evaluation_loss:.4f}__Accuracy_{model_evaluation_accuracy:.4f}.h5'
convlstm_model.save(model_file_name)

print(f"Model saved as {model_file_name}")

# Save the model
model_file_name = f'convlstm_model__Date_Time_{current_date_time_string}__Loss_{model_evaluation_loss:.4f}__Accuracy_{model_evaluation_accuracy:.4f}.h5'
convlstm_model.save(model_file_name)
print(f"Model saved as {model_file_name}")

import matplotlib.pyplot as plt

# Plot training and validation loss
def plot_training_history(training_history):
    # Plot loss
    plt.figure(figsize=(12, 6))
    plt.plot(training_history.history['loss'], label='Training Loss', color='blue')
    plt.plot(training_history.history['val_loss'], label='Validation Loss', color='orange')
    plt.title('Loss Curve')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

    # Plot accuracy
    plt.figure(figsize=(12, 6))
    plt.plot(training_history.history['accuracy'], label='Training Accuracy', color='green')
    plt.plot(training_history.history['val_accuracy'], label='Validation Accuracy', color='red')
    plt.title('Accuracy Curve')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.show()

# Generate the plots
plot_training_history(convlstm_model_training_history)

from collections import deque

def predict_on_video(video_path, model, sequence_length, image_height, image_width, class_names):
    frames_queue = deque(maxlen=sequence_length)
    video_reader = cv2.VideoCapture(video_path)
    prediction_frames = []

    while True:
        ret, frame = video_reader.read()
        if not ret:
            break

        resized_frame = cv2.resize(frame, (image_width, image_height))
        normalized_frame = resized_frame / 255.0
        frames_queue.append(normalized_frame)

        if len(frames_queue) == sequence_length:
            input_data = np.expand_dims(frames_queue, axis=0)
            predicted_probabilities = model.predict(input_data, verbose=0)[0]
            predicted_label = np.argmax(predicted_probabilities)
            predicted_class = class_names[predicted_label]

            # Annotate frame with prediction
            cv2.putText(frame, f"Action: {predicted_class}", (10, 30),
                        cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)
        prediction_frames.append(frame)

    video_reader.release()
    return prediction_frames

def save_predicted_video(output_frames, output_path, fps):
    height, width, _ = output_frames[0].shape
    video_writer = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'XVID'), fps, (width, height))

    for frame in output_frames:
        video_writer.write(frame)

    video_writer.release()
    print(f"Output video saved at {output_path}")

# Usage Example:
input_video_path = "/content/drive/MyDrive/v-walkingwithdog-g04-c03_RuvvWrf4.avi"  # Replace with your test video path
output_video_path = "/content/drive/MyDrive/output_predicted_video.avi"
fps = 30  # Modify as per your video properties

predicted_frames = predict_on_video(input_video_path, convlstm_model, SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, CLASSES_LIST)
save_predicted_video(predicted_frames, output_video_path, fps)

from moviepy.editor import VideoFileClip

# Display video (adjust duration for longer videos)
output_clip = VideoFileClip(output_video_path)
output_clip.ipython_display(maxduration=1000)

# Evaluate test performance
test_loss, test_accuracy = convlstm_model.evaluate(features_test, labels_test)
print(f"Test Loss: {test_loss:.4f}")
print(f"Test Accuracy: {test_accuracy:.4f}")

from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns

# Predictions on test data
y_pred = np.argmax(convlstm_model.predict(features_test), axis=1)
y_true = np.argmax(labels_test, axis=1)

# Confusion Matrix
conf_matrix = confusion_matrix(y_true, y_pred)
plt.figure(figsize=(10, 8))
sns.heatmap(conf_matrix, annot=True, cmap="Blues", xticklabels=CLASSES_LIST, yticklabels=CLASSES_LIST, fmt="d")
plt.title("Confusion Matrix")
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.show()

# Classification Report
report = classification_report(y_true, y_pred, target_names=CLASSES_LIST)
print("Classification Report:\n", report)

from sklearn.metrics import classification_report, confusion_matrix, cohen_kappa_score
from sklearn.metrics import matthews_corrcoef, log_loss, roc_auc_score, hamming_loss, jaccard_score
from tensorflow.keras.metrics import TopKCategoricalAccuracy
import seaborn as sns
import matplotlib.pyplot as plt

# Predictions
y_pred = np.argmax(convlstm_model.predict(features_test), axis=1)
y_true = np.argmax(labels_test, axis=1)
y_pred_prob = convlstm_model.predict(features_test)

# Classification Report
report = classification_report(y_true, y_pred, target_names=CLASSES_LIST, digits=4)
print("Classification Report:\n", report)

# Confusion Matrix
conf_matrix = confusion_matrix(y_true, y_pred)
plt.figure(figsize=(10, 8))
sns.heatmap(conf_matrix, annot=True, cmap="Blues", xticklabels=CLASSES_LIST, yticklabels=CLASSES_LIST, fmt="d")
plt.title("Confusion Matrix")
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.show()

# Additional Metrics
kappa_score = cohen_kappa_score(y_true, y_pred)
mcc = matthews_corrcoef(y_true, y_pred)
logloss = log_loss(labels_test, y_pred_prob)
roc_auc = roc_auc_score(labels_test, y_pred_prob, multi_class="ovr")
hamming = hamming_loss(y_true, y_pred)
jaccard = jaccard_score(y_true, y_pred, average='weighted')

# Top-3 Accuracy
top_k_metric = TopKCategoricalAccuracy(k=3)
top_k_metric.update_state(labels_test, y_pred_prob)
top_k_accuracy = top_k_metric.result().numpy()

# Print Results
print(f"Cohen's Kappa Score: {kappa_score:.4f}")
print(f"Matthews Correlation Coefficient (MCC): {mcc:.4f}")
print(f"Log Loss: {logloss:.4f}")
print(f"ROC-AUC Score: {roc_auc:.4f}")
print(f"Hamming Loss: {hamming:.4f}")
print(f"Jaccard Score: {jaccard:.4f}")
print(f"Top-3 Accuracy: {top_k_accuracy:.4f}")



# Import necessary libraries
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import SparseCategoricalCrossentropy
from tensorflow.keras.datasets import mnist

# Function to plot metrics
def plot_metrics(model_training_history, metric_name_1, metric_name_2, plot_name):
    """
    Plots training and validation metrics for comparison.

    Args:
    - model_training_history: The history object returned by the model's fit() method.
    - metric_name_1: Name of the training metric to plot (e.g., 'loss' or 'accuracy').
    - metric_name_2: Name of the validation metric to plot (e.g., 'val_loss' or 'val_accuracy').
    - plot_name: Title of the plot.

    Returns:
    - None
    """
    metric_value_1 = model_training_history.history[metric_name_1]
    metric_value_2 = model_training_history.history[metric_name_2]
    epochs = range(1, len(metric_value_1) + 1)

    plt.figure(figsize=(8, 6))
    plt.plot(epochs, metric_value_1, 'blue', label=f'Training {metric_name_1}')
    plt.plot(epochs, metric_value_2, 'red', label=f'Validation {metric_name_2}')
    plt.title(plot_name)
    plt.xlabel('Epochs')
    plt.ylabel(metric_name_1)
    plt.legend()
    plt.grid(True)
    plt.show()

# Load the MNIST dataset
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0  # Normalize the data

# Build a simple neural network model
model = Sequential([
    Flatten(input_shape=(28, 28)),
    Dense(128, activation='relu'),
    Dense(10, activation='softmax')
])

# Compile the model
model.compile(optimizer=Adam(),
              loss=SparseCategoricalCrossentropy(),
              metrics=['accuracy'])

# Train the model and save the history
convistm_model_training_history = model.fit(x_train, y_train, validation_split=0.2, epochs=10)

# Evaluate the model on the test dataset
test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=2)

print(f"Test Loss: {test_loss:.4f}")
print(f"Test Accuracy: {test_accuracy:.4f}")

# Plot metrics for training vs validation
plot_metrics(convistm_model_training_history, 'loss', 'val_loss', 'Training Loss vs Validation Loss')
plot_metrics(convistm_model_training_history, 'accuracy', 'val_accuracy', 'Training Accuracy vs Validation Accuracy')

# Predictions on test data (optional)
y_pred = model.predict(x_test)

from tensorflow.keras.models import load_model

# Save model
model.save('convlstm_model.h5')

import cv2
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import load_model
from google.colab.patches import cv2_imshow  # ‚úÖ Fix for Colab

# Define action class labels
class_labels = ['TaiChi', 'HorseRace', 'WalkingWithDog', 'BaseballPitch']

def detect_action(video_path, model, num_frames=20):
    """
    Detects an action from an uploaded video and predicts using the model.
    """
    import cv2
    import numpy as np
    import tensorflow as tf
    from google.colab.patches import cv2_imshow  # ‚úÖ Colab fix for images

    class_labels = ['TaiChi', 'HorseRace', 'WalkingWithDog', 'BaseballPitch']  # Modify as per your classes

    try:
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            print("‚ùå Error: Cannot open video file")
            return

        frames = []
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        step = max(1, total_frames // num_frames)  # Adjust frame step
        frame_count = 0

        while len(frames) < num_frames:
            ret, frame = cap.read()
            if not ret:
                break

            if frame_count % step == 0 and len(frames) < num_frames:
                resized_frame = cv2.resize(frame, (128, 128))
                normalized_frame = resized_frame / 255.0
                frames.append(normalized_frame)

                # ‚úÖ Show frame preview
                print(f"üñºÔ∏è Frame {len(frames)}")
                cv2_imshow(frame)

            frame_count += 1

        cap.release()

        # Ensure we have exactly `num_frames`
        while len(frames) < num_frames:
            frames.append(frames[-1])

        # Convert to NumPy array (1, 20, 128, 128, 3)
        frames = np.array(frames, dtype=np.float32)
        frames = np.expand_dims(frames, axis=0)  # Add batch dimension
        frames = tf.convert_to_tensor(frames)

        print(f"üìå Processed video shape: {frames.shape}, dtype: {frames.dtype}")

        # üî• Make prediction
        predictions = model.predict(frames)
        predicted_class_index = np.argmax(predictions, axis=1)[0]
        confidence_score = np.max(predictions, axis=1)[0]

        # üéØ Display result
        result = f"üéØ Action: {class_labels[predicted_class_index]}"
        print(f"{result} (Confidence: {confidence_score * 100:.2f}%)")

    except Exception as e:
        print("‚ùå Error processing the video:", str(e))

from tensorflow.keras.models import load_model

# üî• Load trained model (update with correct path if needed)
model_path = "/content/convlstm_model__Date_Time_2025_02_23__Loss_0.6645__Accuracy_0.7451.h5"
model = load_model(model_path)

# üî• Predict action from the uploaded video
detect_action(video_path, model)

print(f"‚úÖ Model actual input shape: {model.input_shape}")

print("Model Summary:")
model.summary()

import os

model_path = "/content/convlstm_model__Date_Time_2025_02_23__Loss_0.6645__Accuracy_0.7451.h5"  # Update path if needed

if os.path.exists(model_path):
    print("‚úÖ Model file found:", model_path)
else:
    print("‚ùå Model file NOT found! Check the path.")

model.summary()

# Example usage
video_path = "/content/drive/MyDrive/UCF4/TaiChi/v_TaiChi_g01_c03.avi"  # Replace with your video path
detect_action(video_path, model)

print(model.input_shape)